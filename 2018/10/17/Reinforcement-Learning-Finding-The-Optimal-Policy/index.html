<html>
  <head>
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



<link href="/lib/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
<link href="/css/html.css" rel="stylesheet" type="text/css">
<link href="/css/header.css" rel="stylesheet" type="text/css">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">



<meta name="description" content="Calculating the optimal policy for a Reinforcement Learning problem">
<meta name="keywords" content="katie clark, reinforcement learning, optimal policy, q value function, q-learning">

<title>Reinforcement Learning Finding The Optimal Policy | Come with me now on a journey through code and data...</title>


<link href="/css/post.css" rel="stylesheet" type="text/css">
<link href="/css/notebook.css" rel="stylesheet" type="text/css">

<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning Finding The Optimal Policy">
<meta property="og:url" content="http://hello-klol.github.io/2018/10/17/Reinforcement-Learning-Finding-The-Optimal-Policy/index.html">
<meta property="og:site_name" content="Come with me now on a journey through code and data...">
<meta property="og:description" content="Calculating the optimal policy for a Reinforcement Learning problem">
<meta property="og:locale" content="English">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$']]}
});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
    <title>Reinforcement Learning Finding The Optimal Policy</title>
  </head>
  <body>
<div class="header">
  <div class="title-bar">
    <span class="site-title">Come with me now on a journey through code and data...</span>
  </div>
  <nav class="site-nav">
    <ul>
      <li class="nav-item">
        <a href="/" rel="section">
          <i class="fa fa-fw fa-home"></i> <br>Home</a>
      </li>
      <li class="nav-item">
        <a href="/about/" rel="section">
          <i class="fa fa-fw fa-child"></i> <br>About</a>
      </li>
      <li class="nav-item">
        <a href="/archives/" rel="section">
          <i class="fa fa-fw fa-book"></i> <br>Blog</a>
      </li>
      <li class="nav-item">
        <a href="https://github.com/hello-klol" rel="external" target="_blank">
          <i class="fa fa-fw fa-github-alt"></i> <br>GitHub</a>
      </li>
      <li class="nav-item">
        <a href="https://twitter.com/_katie_clark_" rel="external" target="_blank">
          <i class="fa fa-fw fa-twitter"></i> <br>Twitter</a>
      </li>
      <li class="nav-item">
        <a href="https://stackoverflow.com/users/1168920/katie" rel="external" target="_blank">
          <i class="fa fa-fw fa-stack-overflow"></i> <br>StackOverflow</a>
      </li>
      <li class="nav-item">
        <a href="/katie-clark-resume.pdf" rel="section"i target="_blank">
          <i class="fa fa-fw fa-user"></i> <br>Resume</a>
      </li>
    </ul>
  </nav>
</div>
<article itemscope itemtype="http://schema.org/Article">
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hello-klol.github.io/">

<span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
  <meta itemprop="name" content="Katie Clark">
  <meta itemprop="description" content="It's a Data Science blog">
  <meta itemprop="image" content="/images/avatar.gif">
</span>

<span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
  <meta itemprop="name" content="Come with me now on a journey through code and data...">
</span>
<span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
  <meta itemprop="name" content="Katie Clark">
  <meta itemprop="description" content="It's a Data Science blog">
  <meta itemprop="image" content="/images/avatar.gif">
</span>
<span>
  <meta itemprop="name" content="Come with me now on a journey through code and data...">
</span>


<header>
  <link itemprop="mainEntityOfPage" href="http://hello-klol.github.io/2018/10/17/Reinforcement-Learning-Finding-The-Optimal-Policy/">
    <h1 class="post-title" itemprop="name headline">Reinforcement Learning Finding The Optimal Policy</h1>
  </link>
  <div class="post-meta">
    <span class="post-meta-icon">
      <i class="fa fa-calendar-o"></i>
    </span>
    <span class="post-meta-text">Posted on</span>
    <time title="Created: 2018-10-17 00:00:00" itemprop="dateCreated datePublished" datetime="2018-10-17T00:00:00Z">2018-10-17</time>
    <div class="post-description">Calculating the optimal policy for a Reinforcement Learning problem</div>
  </div>
</header>

    <div class="post-body" itemprop="articleBody">
      <div class="notebook">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="recap">
<h3>Recap</h3> 
<p>    
<a href="/2018/10/16/Reinforcement-Learning-Written-Mathematically/" target="_blank">Last time</a> I described the definition of Reinforcement Learning as: 
</p>
<p>
Given a particular environment (or game)
</p>
<p>
<span style="margin-left:20px"><i class="fa fa-fw fa-angle-double-right"></i>Find the <b>optimal function for choosing actions</b> in each state.</span>
</p>
<p>
By &quot;optimal&quot; we mean that these actions <i>return the greatest rewards</i> 
</p>
<p>
Also known as the <b>optimal policy $\pi^*$</b>, where 

$$\pi^* = \arg\max_{\pi}\mathbb{E}\left[\sum\limits_{t \ge 0} \gamma^t r_t|\pi\right]$$ 
</p>
<p>
Next we&apos;ll look at how to find this optimal policy.
</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Definitions">Definitions<a class="anchor-link" href="#Definitions">&#xB6;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Sample-Trajectories-or-Paths">Sample Trajectories or Paths<a class="anchor-link" href="#Sample-Trajectories-or-Paths">&#xB6;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A &quot;sample trajectory&quot; or &quot;path&quot; is a route taken through the game-space. Given some initial state $s_t$, it describes every action taken, every reward given, and each new state reached, up until the terminal state.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Following the actions determined by a policy gives a set of trajectories. A trajectory is something like:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$$s_0, a_0, r_0, s_1, a_1, r_1, ...$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Value-Function">Value Function<a class="anchor-link" href="#Value-Function">&#xB6;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can think of the <strong>value function</strong> as a way of evaluating <strong>how good a state is</strong>. It looks at all possible trajectories from this state until the terminal state and calculates the expected cumulative reward given by following the policy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Q-Value-Function">Q-Value Function<a class="anchor-link" href="#Q-Value-Function">&#xB6;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <strong>Q-value function</strong> is a way of evaluating <strong>how good a state-action pair is</strong>. As in, how good is taking a particular action in a given state? A Q-value function is the expected cumulative reward <em>from taking a specific action in a particular state</em>, and then continuing to follow the policy.</p>
<p>So the Q-value function is the expected reward for <em>this action</em> given <em>this state</em> plus the value function for whatever state we end up in.</p>
<p>Separating out this step for calculating the value of an action will become clear beow.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Q*">Q*<a class="anchor-link" href="#Q*">&#xB6;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$Q^*$ is the optimal Q-value function, it returns the <strong>maximum expected cumulative reward</strong> that we can get <strong>from a given state-action pair</strong>. So, the value from taking the best trajectory from the given state, after taking the given action.</p>
<p>$$Q^*(s,a)=\max_{\pi}\mathbb{E}\left[\sum_{t\ge0}\gamma^{t}r_{t}|s_{0}=s,a_{0}=a,\pi\right]$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see this is very similar to our optimal policy function. But instead of returning the best policy, it finds the <em>best reward value</em> available from a particular point in the game.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bellman-Equation">Bellman Equation<a class="anchor-link" href="#Bellman-Equation">&#xB6;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What we are trying to solve is a Bellman Equation. A Bellman Equation writes the value of a decision problem at a certain point in time in terms of the payoff from some initial choices, and the value of the remaining decision problem that results from those initial choices.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$Q^*$ satisfies the Bellman Equation:
$$Q^*(s,a)=\mathbb{E}_{s&apos; \sim \mathcal{E}}\left[r+\gamma\max_{a&apos;}Q^*(s&apos;,a&apos;) | s,a\right]$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To reiterate the Q-value definition, the value of any state-action pair can be calculated as</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
the reward that you will get get by taking this action in this state 
+
the value of the state that you end up in (given by the value function)
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Walking through the above equation:</p>
<ul>
<li>Say we we are in state $s$ and we take action $a$.</li>
<li>And say action $a$ puts us in the state $s&apos;$ (&quot;s prime&quot;)</li>
<li>If we are calculating the optimal Q-value we know that, by definition, as we continue we&apos;re going to play the best action that we can for state $s&apos;$ in the next timestep ($t+1$)</li>
<li>So the Q-value of the next state-action pair, $(s&apos;, a&apos;)$, is going to have the maximum value of all possible actions, given state $s&apos;$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p class="sidenote">Note: The only way to know we are following the best possible path is to know every path available and have the ability to compare the possible rewards of different trajectories. Obviously as the state-space increases this quickly becomes infeasible. We&apos;ll go into this later. For now let&apos;s continue with this definition.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This optimal Q-value function is a recursive sum of the reward for the current action in the current state, plus the optimal Q-value for the following state-action pair (which is the reward for the next action when applied to the next state, chosen by following an optimal policy, plus the Q-value for the next state after that).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The definition includes an expectated value ($\mathbb{E}$) because we have randomness over what state we&apos;ll end up in.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="So-what-about-the-policy?">So what about the policy?<a class="anchor-link" href="#So-what-about-the-policy?">&#xB6;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We&apos;ve been talking about ways of calculating the best path or trajectory through a game from any given state which requires knowing every path available to us. But what we need is to select the policy (function that specifies what action to take given a state) that matches the best action to take in any state, as specified by $Q*$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Value-iteration-algorithm">Value iteration algorithm<a class="anchor-link" href="#Value-iteration-algorithm">&#xB6;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One solution is by a value iteration algorithm, where we use the Bellman equation as an iterative update.
$$Q_{i+1}(s,a) = \mathbb{E}\left[r+\gamma\max_{a&apos;}Q_{i}(s&apos;,a&apos;)|s,a\right]$$
$Q_{i} \mapsto Q^{*}$ as $i \mapsto \infty$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At each step, we refine our approximation of $Q^{*}$ by trying to enforce the Bellman equation. Under some mathematical conditions, we know that this sequence $Q_{i}$ of our Q-function is going to converge to our optimal $Q^{*}$ as $i$ approaches infinity.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As mentioned above, this approach is not feasible when the state-space is too large, like in the example of an Atari game where the state space is the screen of pixels. It&apos;s not really possible to make these computations for the entire state-space, so this solution is not scalable.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="How-can-we-make-the-solution-scalable?">How can we make the solution scalable?<a class="anchor-link" href="#How-can-we-make-the-solution-scalable?">&#xB6;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep Learning! Yay!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Neural networks are really good for <em>approximating</em> answers to really complex calculations. Using a function <em>approximator</em> for $Q(s,a)$  to estimate an action&apos;s value-function is called <strong>Q-Learning</strong>, and using a deep neural network for building that function approximator is called <strong>deep Q-Learning</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>More on that to come &#x1F917;</p>

</div>
</div>
</div>
    </div>
  </div>
    </div>

<footer>
  <div class="post-nav">
    <div class="post-nav-prev post-nav-item">
      <a href="/2018/10/16/Reinforcement-Learning-Written-Mathematically/" rel="prev" title="Reinforcement Learning Written Mathematically">
        <i class="fa fa-chevron-left"></i><div class="post-nav-item">Reinforcement Learning Written Mathematically</div>
      </a>
    </div>
    <span class="post-nav-divider"></span>
    <div class="post-nav-next post-nav-item">
      <a href="/" rel="next" title="">
        <div class="post-nav-item"></div><i class="fa fa-chevron-right"></i>
      </a>
    </div>
  </div>
</footer>
  </div>
</article>
  </body>
</html>
